# -*- coding: utf-8 -*-
"""
Created on Thu Feb 12 10:31:53 2015

@author: ryanterrill
"""

"""
In-Class Markov Chain Exercise
2.10.15
@author: jembrown
"""

"""
Recall from your reading that any irreducible and aperiodic Markov chain has a 
stationary distribution. To convince ourselves that things will converge for 
such a chain with arbitrary transition probabilities, let's give it a try.
Work in pairs for this. It's more fun to be social.
"""

# Paste your Markov chain simulation function below, where the starting state
# is drawn with uniform probability from all possible states. Remember to also
# copy any import statements or other functions on which your simulator is
# dependent.

import scipy

def markovChain(matrix,length):
    chain=[] #make an emtpy list
    seed=scipy.random.random(1) #generate a random number
    states=("A","B")
    if seed<matrix[0][0]: #this generates the first state of the chain
        state=states[0]
    else: 
        state=states[1]
    chain.append(state)
    if length ==1:
        return chain #this returns the chain once the recursive function finishes
    else:
        for state in markovChain(matrix,(length-1)): #recursive function addid a state to the chain for predefined length
            if state=="A": #check the previous state then reference the p value to changew or stay the same
                seed=scipy.random.random(1)
                if seed<=matrix[0][0]:
                    state=="A"
                else:
                    state=="B"
            elif state=="B":
                seed=scipy.random.random(1)
                if seed<=matrix[1][1]:
                    state=="B"
                else:
                    state=="A"
            chain.append(state)
            chain.reverse 
        return chain
                
        
    

# Define a 2x2 transition matrix. For fun, don't make all the probabilities
# equal. Also, don't use any 0s or 1s (to make sure the chain is irreducible
# and aperiodic).

Matrix=[[0.4,0.6],[0.7,0.3]]


# Simulate a single chain for three time steps and print the states

markovChain(Matrix,3)


# Analytically calculate the progression of states for this chain.

import numpy as np

npMatrix=np.matrix("0.4,0.6;0.7,0.3")

# Calculate the probability of observing the state in step 3, given the initial
# state in step 1 (i.e., as if you didn't know the state in step 2).

def matrixMult(matrix,steps):
    return matrix**steps
    




M3=matrixMult(npMatrix,3)

pA3givenA1=M3[0,0]


pA3givenA1

#.5260000

# Now think of the chain progressing in the opposite direction. What is the
# probability of the progression through all 3 states in this direction? How
# does this compare to the original direction?

#Same because my chain was ABA:

#but to rerun it

#now BBA
#forwards

PA3givenB1=M3[1,0]

PA3givenB1

#0.552999999

#backwards
PB3givenA1=M3[0,1]

PB3givenA1

#0.47400000

# Try the same "forward" and "reverse" calculations as above, but with this
# transition matrix:
revMat = [[0.77,0.23],[0.39,0.61]]
# and these starting frequencies for "a" and "b"
# freq(a) = 0.63   freq(b) = 0.37


def markovChain63(matrix,length):
    chain=[] #make an emtpy list
    seed=scipy.random.random(1) #generate a random number
    states=("A","B")
    if seed<0.63: #this generates the first state of the chain
        state=states[0]
    else: 
        state=states[1]
    chain.append(state)
    if length ==1:
        return chain #this returns the chain once the recursive function finishes
    else:
        for state in markovChain(matrix,(length-1)): #recursive function addid a state to the chain for predefined length
            if state=="A": #check the previous state then reference the p value to changew or stay the same
                seed=scipy.random.random(1)
                if seed<=matrix[0][0]:
                    state=="A"
                else:
                    state=="B"
            elif state=="B":
                seed=scipy.random.random(1)
                if seed<=matrix[1][1]:
                    state=="B"
                else:
                    state=="A"
            chain.append(state)
            chain.reverse 
        return chain
        
markovChain63(revMat,3)        

# What is (roughly) true about these probabilities?


#BBA


secondMat=np.matrix("0.77,0.23;0.29,0.61")

thirdStep=matrixMult(secondMat,3)

pA3givenB1=thirdStep[1,0]

pA3givenB1
#0.4354

PB3givenA1=thirdStep[0,1]

PB3givenA1

#0.345222

# Simulate 1,000 replicates  (or 10K if your computer is fast enough) of 25 
# steps. What are the frequencies of the 2 states across replicates through time?


def multiMC63(chains,matrix,length):
    chainMatrix=[]
    if chains==1:
        chainMatrix=markovChain63(matrix,length)
    else:    
        while chains>0:
            ch=markovChain63(matrix,length)
            chainMatrix.append(ch)
            chains=chains-1
    return chainMatrix  
    
    
thousandrun=multiMC63(1000,revMat,25)    
    
# NOTE: Here is a function that reports the frequencies of a state through time 
# for replicate simulations. You'll need to do this several times during this exercise.

def mcStateFreqSum(sims,state="a"):
    """
    Pass this function a list of lists. Each individual list should be the
    states of a discrete-state Markov chain through time (and all the same 
    length). It will return a list containing the frequency of one state 
    ("a" by default) across all simulations through time.
    """
    freqs = []
    for i in range(len(sims[0])):  # Iterate across time steps
        stateCount = 0
        for j in range(len(sims)): # Iterate across simulations
            if sims[j][i] == state:
                stateCount += 1
        freqs.extend([float(stateCount)/float(len(sims))])
    return freqs

# Run replicate simulations 

StateA=mcStateFreqSum(thousandrun,"A")

    
# Summarize the frequency of one state through time


StateA

[0.607,
 0.762,
 0.778,
 0.766,
 0.749,
 0.769,
 0.776,
 0.77,
 0.763,
 0.797,
 0.789,
 0.753,
 0.762,
 0.79,
 0.771,
 0.737,
 0.776,
 0.761,
 0.764,
 0.749,
 0.764,
 0.782,
 0.755,
 0.795,
 0.776]


# What do you notice about the state frequencies through time? Try another round
# of simulations with a different transition matrix. How do the state freq.
# values change?


newMat=[[0.1,0.9],[0.1,0.9]]
secondRun=multiMC63(1000,newMat,25)
StateASecondRun=mcStateFreqSum(secondRun,"A")
StateASecondRun

[0.65,
 0.104,
 0.111,
 0.093,
 0.103,
 0.092,
 0.099,
 0.1,
 0.101,
 0.106,
 0.084,
 0.102,
 0.109,
 0.115,
 0.104,
 0.095,
 0.089,
 0.103,
 0.101,
 0.095,
 0.113,
 0.083,
 0.105,
 0.095,
 0.102]




# Now, calculate a vector of probabilities for the focal state (e.g., 'a')
# based on the transition matrix directly (not by simulation). How do these
# values compare to the simulated frequencies?
secondMat=np.matrix("0.77,0.23;0.29,0.61")
newMatnumpy=np.matrix("0.1,0.9;0.1,0.9")


def MCprobs(timesteps,matrix):
    out=[]
    for i in range(len(timesteps)):
        p=matrix**i
        out.append(p[0,0])
    return(out)    
        
MCprobs(StateA,secondMat)

"""
[1.0,
 0.77000000000000002,
 0.65959999999999996,
 0.59993799999999997,
 0.5620956399999999,
 0.53391696919999998,
 0.51028087457599991,
 0.48901906832727993,
 0.46920312183751822,
 0.45042562359988136,
 0.43249850246731647,
 0.41532640709414448,
 0.39885354529559081,
 0.38304135044897519,
 0.36785908486546259,
 0.3532798728834014,
 0.33927901337831246,
 0.32583324969006044,
 0.31292044218082349,
 0.30051941058444204,
 0.28860984840765813,
 0.27717226833703812,
 0.26618796139682632,
 0.25563896258779401,
 0.24550801992823468]
"""

MCprobs(StateASecondRun,newMatnumpy)



[1.0,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001,
 0.10000000000000001]
 
 
 
 # tha values are super similar
